{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Title\n",
    "\n",
    "This notebook lists all the steps that you need to complete the complete this project. You will need to complete all the TODOs in this notebook as well as in the README and the two python scripts included with the starter code.\n",
    "\n",
    "\n",
    "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of these the TODO's and use more than one TODO code cell to do all your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Collecting smdebug\n",
      "  Downloading smdebug-1.0.12-py2.py3-none-any.whl (270 kB)\n",
      "     -------------------------------------- 270.1/270.1 KB 4.1 MB/s eta 0:00:00\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp310-cp310-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 5.5 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "     -------------------------------------- 895.7/895.7 KB 4.1 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.1-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\biggmaathy\\documents\\image-classification-using-aws-sagemaker\\.env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements_tf.txt (line 1)) (23.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\biggmaathy\\documents\\image-classification-using-aws-sagemaker\\.env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements_tf.txt (line 1)) (58.1.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "     ---------------------------------------- 23.2/23.2 MB 9.1 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.30.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 11.8 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     ------------------------------------- 439.2/439.2 KB 13.8 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 14.0 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 10.7 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.24.2-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "     --------------------------------------- 14.8/14.8 MB 14.6 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 KB 7.3 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\biggmaathy\\documents\\image-classification-using-aws-sagemaker\\.env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow->-r requirements_tf.txt (line 1)) (1.16.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 13.2 MB/s eta 0:00:00\n",
      "Collecting pyinstrument==3.4.2\n",
      "  Downloading pyinstrument-3.4.2-py2.py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.3/83.3 KB 4.9 MB/s eta 0:00:00\n",
      "Collecting boto3>=1.10.32\n",
      "  Downloading boto3-1.26.66-py3-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.7/132.7 KB 8.2 MB/s eta 0:00:00\n",
      "Collecting pyinstrument-cext>=0.2.2\n",
      "  Downloading pyinstrument_cext-0.2.4.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.6/79.6 KB 4.3 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.30.0,>=1.29.66\n",
      "  Downloading botocore-1.29.66-py3-none-any.whl (10.4 MB)\n",
      "     --------------------------------------- 10.4/10.4 MB 12.8 MB/s eta 0:00:00\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Using cached urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\biggmaathy\\documents\\image-classification-using-aws-sagemaker\\.env\\lib\\site-packages (from botocore<1.30.0,>=1.29.66->boto3>=1.10.32->smdebug->-r requirements_tf.txt (line 2)) (2.8.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
      "     -------------------------------------- 177.8/177.8 KB 5.4 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.0.1-cp310-cp310-win_amd64.whl (96 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.2-cp310-cp310-win_amd64.whl (16 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using legacy 'setup.py install' for pyinstrument-cext, since package 'wheel' is not installed.\n",
      "Installing collected packages: tensorboard-plugin-wit, pyinstrument-cext, pyasn1, libclang, flatbuffers, charset-normalizer, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyinstrument, pyasn1-modules, protobuf, oauthlib, numpy, MarkupSafe, markdown, keras, jmespath, idna, grpcio, google-pasta, gast, certifi, cachetools, absl-py, werkzeug, requests, opt-einsum, h5py, google-auth, botocore, astunparse, s3transfer, requests-oauthlib, google-auth-oauthlib, boto3, tensorboard, smdebug, tensorflow-intel, tensorflow\n",
      "  Running setup.py install for pyinstrument-cext: started\n",
      "  Running setup.py install for pyinstrument-cext: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Running setup.py install for pyinstrument-cext did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [5 lines of output]\n",
      "      running install\n",
      "      running build\n",
      "      running build_ext\n",
      "      building 'pyinstrument_cext' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "× Encountered error while trying to install package.\n",
      "╰─> pyinstrument-cext\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\BiggMaathy\\Documents\\Image-Classification-using-AWS-SageMaker\\.env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# install tensorflow, smdebug\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuosParameter, HyperparameterTuner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PlantVillage dataset consists of 54303 images of healthy and unhealthy leaves, divided into 38 categories by species and disease. This dataset data is structure as follows:\n",
    "\n",
    "FeaturesDict({\n",
    "    'image': Image(shape=(None, None, 3), dtype=uint8),\n",
    "    'image/filename': Text(shape=(), dtype=string),\n",
    "    'label': ClassLabel(shape=(), dtype=int64, num_classes=38),\n",
    "})\n",
    "\n",
    "Where image field contains the Image itself, the image/filename contains the image filename and the label contains 38 different categories.\n",
    "\n",
    "For more information see: [An open access repository of images on plant health to enable the development of mobile disease diagnostics](https://arxiv.org/abs/1511.08060).\n",
    "\n",
    "For details about this dataset see: https://www.tensorflow.org/datasets/catalog/plant_village?hl=pt-br\n",
    "\n",
    "Donwload dataset: [Plant_leaf_diseases_dataset_without_augmentation.zip](https://data.mendeley.com/datasets/tywbtsjrjv/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fetch and upload the data to AWS S3\n",
    "\n",
    "# Command to download and unzip data\n",
    "!wget s3://sagemaker-us-east-1-890701353618/plant_leaf_diseases/plant_leaf_diseases_dataset_without_augmentation.zip\n",
    "!unzip plant_leaf_diseases_dataset_without_augmentation.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "local_data_dir = \"/tmp/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "**TODO:** This is the part where you will finetune a pretrained model with hyperparameter tuning. Remember that you have to tune a minimum of two hyperparameters. However you are encouraged to tune more. You are also encouraged to explain why you chose to tune those particular hyperparameters and the ranges.\n",
    "\n",
    "**Note:** You will need to use the `hpo.py` script to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of hyperparameters for the model training\n",
    "hyperparameters_range = {\n",
    "    \"learning_rate\": ContinuosParameter(1e-4, 1e-3),\n",
    "    \"batch_size\": CategoricalParameter([32, 64, 128, 256, 512])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow estimator for training the model\n",
    "estimator = TensorFlow(\n",
    "    # Specify the entry point script for the training job\n",
    "    entry_point= \"train_model.py\",\n",
    "    # Assign the AWS IAM role for the training job\n",
    "    role = role,\n",
    "    # Set the number of instances for the training job to 1\n",
    "    instance_count=1,\n",
    "    # Specify the instance type for the training job\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    # Set the source directory for the training scripts\n",
    "    source_dir=\"scripts\",\n",
    "    # Set the version of TensorFlow to use\n",
    "    framework_version=\"2.3.1\",\n",
    "    # Set the version of Python to use\n",
    "    py_version=\"py37\",\n",
    "    # Set the hyperparameters for the training job\n",
    "    hyperparameters={\n",
    "        \"batch-size\": 512,\n",
    "        \"epochs\": 20\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define a objective metric to tune. Note: the entry point script must return the same metric specified here\n",
    "objective_metric_name = \"average test loss\"\n",
    "\n",
    "# Define the objective (Minimize or Maximize)\n",
    "objective_type = \"Minimize\"\n",
    "\n",
    "# Define the metric definitions such as name and a regular expression needed to \n",
    "# extract the metric from the CloudWatch logs of the training job\n",
    "metric_definitions = [{'Name': 'average test loss', 'Regex': 'average test loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "# Define a HyperparameterTuner \n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameters_range,\n",
    "    metric_definitions,\n",
    "    max_jobs=4, # Number of training jobs to run\n",
    "    max_parallel_jobs=2, # Number of training jobs to run in parallel\n",
    "    objective_type=objective_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a prefix for the data stored in the S3 bucket\n",
    "prefix = \"plants\"\n",
    "\n",
    "# Get the default S3 bucket for the current AWS session\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Upload the local data to the S3 bucket with the specified prefix\n",
    "loc = session.upload_data(path=local_data_dir, bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "# Define the input channels for the training and testing data\n",
    "channels = {\"training\": loc, \"testing\": loc}\n",
    "\n",
    "# Fit the tuner to the input data channels\n",
    "tuner.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimators\n",
    "best_estimator = tuner.best_estimator()\n",
    "\n",
    "#Get the hyperparameters of the best trained model\n",
    "hypers = best_estimator.hyperparameters()\n",
    "batch_size = hypers[\"batch-size\"] # return a string\n",
    "learning_rate = hypers[\"learning-rate\"] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy best model to a endpoint\n",
    "predictor = tuner.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n",
    "\n",
    "# Delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Profiling and Debugging\n",
    "TODO: Using the best hyperparameters, create and finetune a new model\n",
    "\n",
    "**Note:** You will need to use the `train_model.py` script to perform model profiling and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyperparameter from best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up debugging and profiling rules and hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and fit an estimator\n",
    "\n",
    "estimator = # TODO: Your estimator here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a debugging output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display the profiler output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Deploy your model to an endpoint\n",
    "\n",
    "predictor=estimator.deploy() # TODO: Add your deployment configuration like instance type and number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run an prediction on the endpoint\n",
    "\n",
    "image = # TODO: Your code to load and preprocess image to send to endpoint for prediction\n",
    "response = predictor.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remember to shutdown/delete your endpoint once your work is done\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e33795c2df57a29cb59fe5bf954db838a3060a403befcc4ba89b8767caa0c1e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
